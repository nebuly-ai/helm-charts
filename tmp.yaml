---
# Source: nebuly-platform/charts/strimzi-kafka-operator/templates/010-ServiceAccount-strimzi-cluster-operator.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: strimzi-cluster-operator
  namespace: default
  labels:
    app: strimzi
    chart: strimzi-kafka-operator-0.43.0
    component: service-account
    release: release-name
    heritage: Helm
---
# Source: nebuly-platform/templates/auth-service_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-auth-service
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-auth-service
  annotations:
    nebuly.com/release-name: release-name
stringData:
  postgres-user: ""
  postgres-password: ""
  azure-client-id: ""
  azure-client-secret: ""
  microsoft-oauth-tenant-id: ""
  microsoft-oauth-client-id: ""
  microsoft-oauth-client-secret: ""
  okta-oauth-client-id: ""
  okta-oauth-client-secret: ""
---
# Source: nebuly-platform/templates/backend_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-backend
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-backend
  annotations:
    nebuly.com/release-name: release-name
stringData:
  analytic-database-user: ""
  analytic-database-password: ""
  openai-api-key: ""
---
# Source: nebuly-platform/templates/ingestion-worker_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-ingestion-worker
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-ingestion-worker
  annotations:
    nebuly.com/release-name: release-name
stringData:
  analytic-database-user: ""
  analytic-database-password: ""
  openai-api-key: ""
---
# Source: nebuly-platform/templates/model-registry_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-model-registry
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
  annotations:
    nebuly.com/release-name: release-name
stringData:
  azure-client-id: ""
  azure-client-secret: ""
  aws-access-key-id: ""
  aws-secret-access-key: ""
---
# Source: nebuly-platform/templates/models-sync_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-models-sync
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: models-sync
  annotations:
    nebuly.com/release-name: release-name
stringData:
  source-azure-client-id: ""
  source-azure-client-secret: ""
---
# Source: nebuly-platform/charts/strimzi-kafka-operator/templates/050-ConfigMap-strimzi-cluster-operator.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: strimzi-cluster-operator
  namespace: default
  labels:
    app: strimzi
    chart: strimzi-kafka-operator-0.43.0
    component: logging-config-map
    release: release-name
    heritage: Helm
data:
  log4j2.properties: |
    name = COConfig
    monitorInterval = 30

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n

    rootLogger.level = ${env:STRIMZI_LOG_LEVEL:-INFO}
    rootLogger.appenderRefs = stdout
    rootLogger.appenderRef.console.ref = STDOUT

    # Kafka AdminClient logging is a bit noisy at INFO level
    logger.kafka.name = org.apache.kafka
    logger.kafka.level = WARN

    # Zookeeper is very verbose even on INFO level -> We set it to WARN by default
    logger.zookeepertrustmanager.name = org.apache.zookeeper
    logger.zookeepertrustmanager.level = WARN

    # Keeps separate level for Netty logging -> to not be changed by the root logger
    logger.netty.name = io.netty
    logger.netty.level = INFO
---
# Source: nebuly-platform/templates/frontend_configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-frontend
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-frontend
  annotations:
    nebuly.com/release-name: release-name
data:
  env.json: |
    {
      "GIT_REF": "${GIT_REF}",
      "LOGIN_MODES": "password",
      "DEPLOYMENT_MODE": "self-hosted",
      "NEXT_PUBLIC_AUTH_MODE": "custom",
      "NEXT_PUBLIC_API_URL": "https://platform.azure.testing.nebuly.com/backend",
      "NEXT_PUBLIC_AUTH_API_URL": "https://platform.azure.testing.nebuly.com/backend/auth",
      "NEXT_PUBLIC_SENTRY_ENABLED": "false",
      "NEXT_PUBLIC_SENTRY_DSN": "",
      "NEXT_PUBLIC_SENTRY_ENVIRONMENT": "",
      "NEXT_PUBLIC_SENTRY_TRACES_SAMPLE_RATE": "0",
      "NEXT_PUBLIC_SENTRY_REPLAY_SESSION_SAMPLE_RATE": "0",
      "NEXT_PUBLIC_ENABLE_LLM_ISSUES_V2": "true",
      "NEXT_PUBLIC_APP_FAVICON": "/favicon.svc",
      "NEXT_PUBLIC_APP_TITLE": "Nebuly",
      "NEXT_PUBLIC_ENABLE_TRANSLATIONS": "false",
      "NEXT_PUBLIC_ENABLE_ADD_MEMBER": "false",
      "NEXT_PUBLIC_ENABLE_CONVERSATIONS" : "true",
      "NEXT_PUBLIC_DEFAULT_AGGREGATION": "interaction",
      "NEXT_PUBLIC_ENABLE_AI_SUMMARY": "false",
      "NEXT_PUBLIC_ENABLE_OLD_RISKY_BEHAVIOR": "false"
    }
---
# Source: nebuly-platform/charts/strimzi-kafka-operator/templates/020-ClusterRole-strimzi-cluster-operator-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: strimzi-cluster-operator-namespaced
  labels:
    app: strimzi
    chart: strimzi-kafka-operator-0.43.0
    component: role
    release: release-name
    heritage: Helm
rules:
# Resources in this role are used by the operator based on an operand being deployed in some namespace. When needed, you
# can deploy the operator as a cluster-wide operator. But grant the rights listed in this role only on the namespaces
# where the operands will be deployed. That way, you can limit the access the operator has to other namespaces where it
# does not manage any clusters.
- apiGroups:
  - "rbac.authorization.k8s.io"
  resources:
    # The cluster operator needs to access and manage rolebindings to grant Strimzi components cluster permissions
  - rolebindings
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - "rbac.authorization.k8s.io"
  resources:
    # The cluster operator needs to access and manage roles to grant the entity operator permissions
  - roles
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - ""
  resources:
    # The cluster operator needs to access and delete pods, this is to allow it to monitor pod health and coordinate rolling updates
  - pods
    # The cluster operator needs to access and manage service accounts to grant Strimzi components cluster permissions
  - serviceaccounts
    # The cluster operator needs to access and manage config maps for Strimzi components configuration
  - configmaps
    # The cluster operator needs to access and manage services and endpoints to expose Strimzi components to network traffic
  - services
  - endpoints
    # The cluster operator needs to access and manage secrets to handle credentials
  - secrets
    # The cluster operator needs to access and manage persistent volume claims to bind them to Strimzi components for persistent data
  - persistentvolumeclaims
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - "apps"
  resources:
    # The cluster operator needs to access and manage deployments to run deployment based Strimzi components
  - deployments
    # The cluster operator needs to access and manage stateful sets to run stateful sets based Strimzi components
  - statefulsets
    # The cluster operator needs to access replica-sets to manage Strimzi components and to determine error states
  - replicasets
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - "apps"
  resources:
    # The Cluster Operator needs to scale Deployments while migrating Connect and Mirror Maker 2 clusters from Deployments to StrimziPodSets
  - deployments/scale
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - "events.k8s.io" # new events api, used by cluster operator
  resources:
    # The cluster operator needs to be able to create events
  - events
  verbs:
  - create
- apiGroups:
    # Kafka Connect Build on OpenShift requirement
  - build.openshift.io
  resources:
  - buildconfigs
  - buildconfigs/instantiate
  - builds
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - networking.k8s.io
  resources:
    # The cluster operator needs to access and manage network policies to lock down communication between Strimzi components
  - networkpolicies
    # The cluster operator needs to access and manage ingresses which allow external access to the services in a cluster
  - ingresses
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - route.openshift.io
  resources:
    # The cluster operator needs to access and manage routes to expose Strimzi components for external access
  - routes
  - routes/custom-host
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - image.openshift.io
  resources:
  # The cluster operator needs to verify the image stream when used for Kafka Connect image build
  - imagestreams
  verbs:
  - get
- apiGroups:
  - policy
  resources:
    # The cluster operator needs to access and manage pod disruption budgets this limits the number of concurrent disruptions
    # that a Strimzi component experiences, allowing for higher availability
  - poddisruptionbudgets
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
---
# Source: nebuly-platform/charts/strimzi-kafka-operator/templates/021-ClusterRole-strimzi-cluster-operator-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: strimzi-cluster-operator-global
  labels:
    app: strimzi
    chart: strimzi-kafka-operator-0.43.0
    component: role
    release: release-name
    heritage: Helm
rules:
- apiGroups:
  - "rbac.authorization.k8s.io"
  resources:
    # The cluster operator needs to create and manage cluster role bindings in the case of an install where a user
    # has specified they want their cluster role bindings generated
  - clusterrolebindings
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - storage.k8s.io
  resources:
    # The cluster operator requires "get" permissions to view storage class details
    # This is because only a persistent volume of a supported storage class type can be resized
  - storageclasses
  verbs:
  - get
- apiGroups:
    - ""
  resources:
    # The cluster operator requires "list" permissions to view all nodes in a cluster
    # The listing is used to determine the node addresses when NodePort access is configured
    # These addresses are then exposed in the custom resource states
  - nodes
  verbs:
  - list
---
# Source: nebuly-platform/charts/strimzi-kafka-operator/templates/022-ClusterRole-strimzi-cluster-operator-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: strimzi-cluster-operator-leader-election
  labels:
    app: strimzi
    chart: strimzi-kafka-operator-0.43.0
    component: role
    release: release-name
    heritage: Helm
rules:
- apiGroups:
  - coordination.k8s.io
  resources:
    # The cluster operator needs to access and manage leases for leader election
    # The "create" verb cannot be used with "resourceNames"
  - leases
  verbs:
  - create
- apiGroups:
  - coordination.k8s.io
  resources:
    # The cluster operator needs to access and manage leases for leader election
  - leases
  resourceNames:
    # The default RBAC files give the operator only access to the Lease resource names strimzi-cluster-operator
    # If you want to use another resource name or resource namespace, you have to configure the RBAC resources accordingly
  - strimzi-cluster-operator
  verbs:
  - get
  - list
  - watch
  - delete
  - patch
  - update
---
# Source: nebuly-platform/charts/strimzi-kafka-operator/templates/023-ClusterRole-strimzi-cluster-operator-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: strimzi-cluster-operator-watched
  labels:
    app: strimzi
    chart: strimzi-kafka-operator-0.43.0
    component: role
    release: release-name
    heritage: Helm
rules:
# Resources in this role are being watched by the operator. When operator is deployed as cluster-wide, these permissions
# need to be granted to the operator on a cluster wide level as well, even if the operands will be deployed only in
# few of the namespaces in given cluster. This is required to set up the Kubernetes watches and informers.
# Note: The rights included in this role might change in the future
- apiGroups:
  - ""
  resources:
    # The cluster operator needs to access and delete pods, this is to allow it to monitor pod health and coordinate rolling updates
  - pods
  verbs:
  - watch
  - list
- apiGroups:
  - "kafka.strimzi.io"
  resources:
  # The Cluster Operator operates the Strimzi custom resources
  - kafkas
  - kafkanodepools
  - kafkaconnects
  - kafkaconnectors
  - kafkamirrormakers
  - kafkabridges
  - kafkamirrormaker2s
  - kafkarebalances
  verbs:
  - get
  - list
  - watch
  - create
  - patch
  - update
- apiGroups:
  - "kafka.strimzi.io"
  resources:
  # The Cluster Operator needs to manage the status of the Strimzi custom resources
  - kafkas/status
  - kafkanodepools/status
  - kafkaconnects/status
  - kafkaconnectors/status
  - kafkamirrormakers/status
  - kafkabridges/status
  - kafkamirrormaker2s/status
  - kafkarebalances/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - "core.strimzi.io"
  resources:
  # The cluster operator uses StrimziPodSets to manage the Kafka and ZooKeeper pods
  - strimzipodsets
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - "core.strimzi.io"
  resources:
  # The Cluster Operator needs to manage the status of the StrimziPodSet custom resource
  - strimzipodsets/status
  verbs:
  - get
  - patch
  - update
---
# Source: nebuly-platform/charts/strimzi-kafka-operator/templates/030-ClusterRole-strimzi-kafka-broker.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: strimzi-kafka-broker
  labels:
    app: strimzi
    chart: strimzi-kafka-operator-0.43.0
    component: broker-role
    release: release-name
    heritage: Helm
rules:
- apiGroups:
  - ""
  resources:
    # The Kafka Brokers require "get" permissions to view the node they are on
    # This information is used to generate a Rack ID that is used for High Availability configurations
  - nodes
  verbs:
  - get
---
# Source: nebuly-platform/charts/strimzi-kafka-operator/templates/031-ClusterRole-strimzi-entity-operator.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: strimzi-entity-operator
  labels:
    app: strimzi
    chart: strimzi-kafka-operator-0.43.0
    component: entity-operator-role
    release: release-name
    heritage: Helm
rules:
- apiGroups:
  - "kafka.strimzi.io"
  resources:
    # The Entity Operator contains the Topic Operator which needs to access and manage KafkaTopic resources
  - kafkatopics
  verbs:
  - get
  - list
  - watch
  - create
  - patch
  - update
  - delete
- apiGroups:
  - "kafka.strimzi.io"
  resources:
    # The Entity Operator contains the User Operator which needs to access and manage KafkaUser resources
  - kafkausers
  verbs:
  - get
  - list
  - watch
  - create
  - patch
  - update
- apiGroups:
  - "kafka.strimzi.io"
  resources:
    # The Entity Operator contains the Topic Operator which needs to access and manage KafkaTopic resources
  - kafkatopics/status
    # The Entity Operator contains the User Operator which needs to access and manage KafkaUser resources
  - kafkausers/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - ""
  resources:
    # The entity operator user-operator needs to access and manage secrets to store generated credentials
  - secrets
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
---
# Source: nebuly-platform/charts/strimzi-kafka-operator/templates/033-ClusterRole-strimzi-kafka-client.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: strimzi-kafka-client
  labels:
    app: strimzi
    chart: strimzi-kafka-operator-0.43.0
    component: client-role
    release: release-name
    heritage: Helm
rules:
- apiGroups:
  - ""
  resources:
    # The Kafka clients (Connect, Mirror Maker, etc.) require "get" permissions to view the node they are on
    # This information is used to generate a Rack ID (client.rack option) that is used for consuming from the closest
    # replicas when enabled
  - nodes
  verbs:
  - get
---
# Source: nebuly-platform/charts/strimzi-kafka-operator/templates/021-ClusterRoleBinding-strimzi-cluster-operator.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: strimzi-cluster-operator
  labels:
    app: strimzi
    chart: strimzi-kafka-operator-0.43.0
    component: role-binding
    release: release-name
    heritage: Helm
subjects:
  - kind: ServiceAccount
    name: strimzi-cluster-operator
    namespace: default
roleRef:
  kind: ClusterRole
  name: strimzi-cluster-operator-global
  apiGroup: rbac.authorization.k8s.io
---
# Source: nebuly-platform/charts/strimzi-kafka-operator/templates/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: strimzi-cluster-operator-kafka-broker-delegation
  labels:
    app: strimzi
    chart: strimzi-kafka-operator-0.43.0
    component: broker-role-binding
    release: release-name
    heritage: Helm
# The Kafka broker cluster role must be bound to the cluster operator service account so that it can delegate the cluster role to the Kafka brokers.
# This must be done to avoid escalating privileges which would be blocked by Kubernetes.
subjects:
  - kind: ServiceAccount
    name: strimzi-cluster-operator
    namespace: default
roleRef:
  kind: ClusterRole
  name: strimzi-kafka-broker
  apiGroup: rbac.authorization.k8s.io
---
# Source: nebuly-platform/charts/strimzi-kafka-operator/templates/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: strimzi-cluster-operator-kafka-client-delegation
  labels:
    app: strimzi
    chart: strimzi-kafka-operator-0.43.0
    component: client-role-binding
    release: release-name
    heritage: Helm
# The Kafka clients cluster role must be bound to the cluster operator service account so that it can delegate the
# cluster role to the Kafka clients using it for consuming from closest replica.
# This must be done to avoid escalating privileges which would be blocked by Kubernetes.
subjects:
  - kind: ServiceAccount
    name: strimzi-cluster-operator
    namespace: default
roleRef:
  kind: ClusterRole
  name: strimzi-kafka-client
  apiGroup: rbac.authorization.k8s.io
---
# Source: nebuly-platform/charts/strimzi-kafka-operator/templates/020-RoleBinding-strimzi-cluster-operator.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: strimzi-cluster-operator
  namespace: default
  labels:
    app: strimzi
    chart: strimzi-kafka-operator-0.43.0
    component: role-binding
    release: release-name
    heritage: Helm
subjects:
  - kind: ServiceAccount
    name: strimzi-cluster-operator
    namespace: default
roleRef:
  kind: ClusterRole
  name: strimzi-cluster-operator-namespaced
  apiGroup: rbac.authorization.k8s.io
---
# Source: nebuly-platform/charts/strimzi-kafka-operator/templates/022-RoleBinding-strimzi-cluster-operator.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: strimzi-cluster-operator-leader-election
  namespace: default
  labels:
    app: strimzi
    chart: strimzi-kafka-operator-0.43.0
    component: role-binding
    release: release-name
    heritage: Helm
subjects:
  - kind: ServiceAccount
    name: strimzi-cluster-operator
    namespace: default
roleRef:
  kind: ClusterRole
  name: strimzi-cluster-operator-leader-election
  apiGroup: rbac.authorization.k8s.io
---
# Source: nebuly-platform/charts/strimzi-kafka-operator/templates/023-RoleBinding-strimzi-cluster-operator.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: strimzi-cluster-operator-watched
  namespace: default
  labels:
    app: strimzi
    chart: strimzi-kafka-operator-0.43.0
    component: role-binding
    release: release-name
    heritage: Helm
subjects:
  - kind: ServiceAccount
    name: strimzi-cluster-operator
    namespace: default
roleRef:
  kind: ClusterRole
  name: strimzi-cluster-operator-watched
  apiGroup: rbac.authorization.k8s.io
---
# Source: nebuly-platform/charts/strimzi-kafka-operator/templates/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: strimzi-cluster-operator-entity-operator-delegation
  namespace: default
  labels:
    app: strimzi
    chart: strimzi-kafka-operator-0.43.0
    component: entity-operator-role-binding
    release: release-name
    heritage: Helm
# The Entity Operator cluster role must be bound to the cluster operator service account so that it can delegate the cluster role to the Entity Operator.
# This must be done to avoid escalating privileges which would be blocked by Kubernetes.
subjects:
  - kind: ServiceAccount
    name: strimzi-cluster-operator
    namespace: default
roleRef:
  kind: ClusterRole
  name: strimzi-entity-operator
  apiGroup: rbac.authorization.k8s.io
---
# Source: nebuly-platform/templates/auth-service_service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-auth-service
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-auth-service
  annotations:
    nebuly.com/release-name: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-auth-service
---
# Source: nebuly-platform/templates/backend_service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-backend
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-backend
  annotations:
    nebuly.com/release-name: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-backend
---
# Source: nebuly-platform/templates/event-ingestion_service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-event-ingestion
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-event-ingestion
  annotations:
    nebuly.com/release-name: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-event-ingestion
---
# Source: nebuly-platform/templates/frontend_service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-frontend
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-frontend
  annotations:
    nebuly.com/release-name: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-frontend
---
# Source: nebuly-platform/templates/ingestion-worker_service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-ingestion-worker
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-ingestion-worker
  annotations:
    nebuly.com/release-name: release-name
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-ingestion-worker
---
# Source: nebuly-platform/charts/strimzi-kafka-operator/templates/060-Deployment-strimzi-cluster-operator.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: strimzi-cluster-operator
  namespace: default
  labels:
    app: strimzi
    chart: strimzi-kafka-operator-0.43.0
    component: deployment
    release: release-name
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      name: strimzi-cluster-operator
      strimzi.io/kind: cluster-operator
  template:
    metadata:
      labels:
        name: strimzi-cluster-operator
        strimzi.io/kind: cluster-operator
    spec:
      serviceAccountName: strimzi-cluster-operator
      volumes:
        - name: strimzi-tmp
          emptyDir:
            medium: Memory
            sizeLimit: 1Mi
        - name: co-config-volume
          configMap:
            name: strimzi-cluster-operator
      containers:
        - name: strimzi-cluster-operator
          image: quay.io/strimzi/operator:0.43.0
          ports:
            - containerPort: 8080
              name: http
          args:
            - /opt/strimzi/bin/cluster_operator_run.sh
          volumeMounts:
            - name: strimzi-tmp
              mountPath: /tmp
            - name: co-config-volume
              mountPath: /opt/strimzi/custom-config/
          env:
            - name: STRIMZI_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS
              value: "120000"
            - name: STRIMZI_OPERATION_TIMEOUT_MS
              value: "300000"
            - name: STRIMZI_DEFAULT_KAFKA_EXPORTER_IMAGE
              value: quay.io/strimzi/kafka:0.43.0-kafka-3.8.0
            - name: STRIMZI_DEFAULT_CRUISE_CONTROL_IMAGE
              value: quay.io/strimzi/kafka:0.43.0-kafka-3.8.0
            - name: STRIMZI_KAFKA_IMAGES
              value: |                 
                3.7.0=quay.io/strimzi/kafka:0.43.0-kafka-3.7.0
                3.7.1=quay.io/strimzi/kafka:0.43.0-kafka-3.7.1
                3.8.0=quay.io/strimzi/kafka:0.43.0-kafka-3.8.0
            - name: STRIMZI_KAFKA_CONNECT_IMAGES
              value: |                 
                3.7.0=quay.io/strimzi/kafka:0.43.0-kafka-3.7.0
                3.7.1=quay.io/strimzi/kafka:0.43.0-kafka-3.7.1
                3.8.0=quay.io/strimzi/kafka:0.43.0-kafka-3.8.0
            - name: STRIMZI_KAFKA_MIRROR_MAKER_IMAGES
              value: |                 
                3.7.0=quay.io/strimzi/kafka:0.43.0-kafka-3.7.0
                3.7.1=quay.io/strimzi/kafka:0.43.0-kafka-3.7.1
                3.8.0=quay.io/strimzi/kafka:0.43.0-kafka-3.8.0
            - name: STRIMZI_KAFKA_MIRROR_MAKER_2_IMAGES
              value: |                 
                3.7.0=quay.io/strimzi/kafka:0.43.0-kafka-3.7.0
                3.7.1=quay.io/strimzi/kafka:0.43.0-kafka-3.7.1
                3.8.0=quay.io/strimzi/kafka:0.43.0-kafka-3.8.0
            - name: STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE
              value: quay.io/strimzi/operator:0.43.0
            - name: STRIMZI_DEFAULT_USER_OPERATOR_IMAGE
              value: quay.io/strimzi/operator:0.43.0
            - name: STRIMZI_DEFAULT_KAFKA_INIT_IMAGE
              value: quay.io/strimzi/operator:0.43.0
            - name: STRIMZI_DEFAULT_KAFKA_BRIDGE_IMAGE
              value: quay.io/strimzi/kafka-bridge:0.30.0
            - name: STRIMZI_DEFAULT_KANIKO_EXECUTOR_IMAGE
              value: quay.io/strimzi/kaniko-executor:0.43.0
            - name: STRIMZI_DEFAULT_MAVEN_BUILDER
              value: quay.io/strimzi/maven-builder:0.43.0
            - name: STRIMZI_OPERATOR_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            
            - name: STRIMZI_FEATURE_GATES
              value: ""
            - name: STRIMZI_LEADER_ELECTION_ENABLED
              value: "true"
            - name: STRIMZI_LEADER_ELECTION_LEASE_NAME
              value: "strimzi-cluster-operator"
            - name: STRIMZI_LEADER_ELECTION_LEASE_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: STRIMZI_LEADER_ELECTION_IDENTITY
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          livenessProbe:
            httpGet:
              path: /healthy
              port: http
            initialDelaySeconds: 10
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 10
            periodSeconds: 30
          resources:
            limits:
              cpu: 1000m
              memory: 384Mi
            requests:
              cpu: 200m
              memory: 384Mi
---
# Source: nebuly-platform/templates/auth-service_deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-auth-service
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-auth-service
  annotations:
    nebuly.com/release-name: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: nebuly-platform
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/part-of: nebuly-platform
      app.kubernetes.io/component: nebuly-auth-service
  template:
    metadata:
      labels:
        app.kubernetes.io/name: nebuly-platform
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/part-of: nebuly-platform
        app.kubernetes.io/component: nebuly-auth-service
    spec:
      serviceAccountName: default
      imagePullSecrets:
        - name: nebuly-docker-pull
      securityContext:
        runAsNonRoot: true
      containers:
        - name: nebuly-platform
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
          image: "ghcr.io/nebuly-ai/nebuly-tenant-registry:v1.13.3"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          env:
            # Database
            - name: "POSTGRES_SERVER"
              value: nbltstnebulydb3.postgres.database.azure.com
            - name: "POSTGRES_DB"
              value: auth
            - name: "POSTGRES_USER"
              valueFrom:
                secretKeyRef:
                  name: nebuly-platform-credentials
                  key: db-username
            - name: "POSTGRES_PASSWORD"
              valueFrom:
                secretKeyRef:
                  name: nebuly-platform-credentials
                  key: db-password
            # Secrets Store
            - name: "SECRETS_STORE_KIND"
              value: database
            - name: "AZURE_CLIENT_ID"
              valueFrom:
                secretKeyRef:
                  name: release-name-auth-service
                  key: azure-client-id
            - name: "AZURE_CLIENT_SECRET"
              valueFrom:
                secretKeyRef:
                  name: release-name-auth-service
                  key: azure-client-secret
            - name: "AZURE_TENANT_ID"
              value: 
            - name: "AZURE_KEYVAULT_URL"
              value: 
            # OTEL
            - name: "OTEL_ENABLED"
              value: "false"
            - name: "OTEL_SERVICE_NAME"
              value: "nebuly-auth-service"
            - name: "OTEL_EXPORTER_OTLP_TRACES_ENDPOINT"
              value: "http://contrib-collector.otel:4317"
            - name: "OTEL_EXPORTER_OTLP_METRICS_ENDPOINT"
              value: "http://contrib-collector.otel:4317"
            - name: "OTEL_METRICS_EXPORTER"
              value: "otlp"
            # Sentry
            - name: SENTRY_ENABLED
              value: "false"
            - name: SENTRY_ENVIRONMENT
              value: ""
            - name: SENTRY_DSN
              value: ""
            - name: SENTRY_TRACES_SAMPLE_RATE
              value: "0"
            - name: SENTRY_PROFILES_SAMPLE_RATE
              value: "0"
            # CORS
            # Auth
            - name: "JWT_SIGNING_KEY_PATH"
              value: "/app/resources/private-key.pem"
            - name: "ADMIN_USER_ENABLED"
              value: "false"
            - name: "ADMIN_USER_USERNAME"
              value: admin@nebuly.ai
            - name: "ADMIN_USER_PASSWORD"
              value: admin
            
            
            - name: "SLACK_NOTIFICATIONS_ENABLED"
              value: "false"
            - name: "TEAMS_NOTIFICATIONS_ENABLED"
              value: "false"
            - name: "TELEMETRY_OVERRIDE_TENANT"
              value: "release-name"
            # Misc
            - name: "ENV"
              value: "prod"
            - name: "SERVER_PORT"
              value: "8080"
            - name: "PROJECT_NAME"
              value: "auth-service"
            - name: "DEVELOPMENT_MODE"
              value: "false"
            - name: "STRIPE_ENABLED"
              value: "false"

          livenessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 10
            periodSeconds: 15
          readinessProbe:
            httpGet:
              path: /readyz
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
          resources:
            limits:
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          volumeMounts:
            - name: auth-service-secret
              readOnly: true
              mountPath: "/app/resources/private-key.pem"
              subPath: jwt-signing-key
            - mountPath: /mnt/secrets-store
              name: secrets-store
              readOnly: true
      volumes:
        - name: auth-service-secret
          secret:
            secretName: nebuly-platform-credentials
        - csi:
            driver: secrets-store.csi.k8s.io
            readOnly: true
            volumeAttributes:
              secretProviderClass: nebuly-platform
          name: secrets-store
---
# Source: nebuly-platform/templates/backend_deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-backend
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-backend
  annotations:
    nebuly.com/release-name: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: nebuly-platform
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/part-of: nebuly-platform
      app.kubernetes.io/component: nebuly-backend
  template:
    metadata:
      labels:
        app.kubernetes.io/name: nebuly-platform
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/part-of: nebuly-platform
        app.kubernetes.io/component: nebuly-backend
    spec:
      serviceAccountName: default
      imagePullSecrets:
        - name: nebuly-docker-pull
      securityContext:
        runAsNonRoot: true
      initContainers:
        - name: migrations
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
          image: "ghcr.io/nebuly-ai/nebuly-backend:v1.57.29"
          imagePullPolicy: IfNotPresent
          command:
            - python
            - app/backend_pre_start.py
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          env:
            # Common env
            
            # Database
            - name: "MULTI_TENANCY_MODE"
              value: "dynamic_schema"
            - name: "ANALYTICS_USER"
              valueFrom:
                secretKeyRef:
                  name: nebuly-platform-credentials
                  key: db-username
            - name: "ANALYTICS_PASSWORD"
              valueFrom:
                secretKeyRef:
                  name: nebuly-platform-credentials
                  key: db-password
            - name: ANALYTICS_DB
              value: "analytics"
            - name: ANALYTICS_SERVER
              value: "nbltstnebulydb3.postgres.database.azure.com"
            # Misc (TODO: remove)
            - name: TENANT
              value: "release-name"
            # OTEL
            - name: OTEL_ENABLED
              value: "false"
            - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
              value: "http://contrib-collector.otel:4317"
            - name: OTEL_EXPORTER_OTLP_METRICS_ENDPOINT
              value: "http://contrib-collector.otel:4317"
            - name: OTEL_METRICS_EXPORTER
              value: "otlp"
            # Old Auth0 stuff, to be removed as soon as we leave Auth0
            - name: AUTH0_ENABLED
              value: "false"
            - name: OAUTH_CLIENT_ID
              value: ""
            - name: OAUTH_CLIENT_SECRET
              value: ""
            - name: OAUTH_DOMAIN
              value: ""
            - name: OAUTH_AUDIENCE
              value: ""
            - name: AUTH0_JWKS_URL
              value: ""
            - name: AUTH0_DATABASE_CONNECTION
              value: ""
            # Oauth
            - name: OAUTH_JWKS_URL
              value: "http://release-name-auth-service:80/auth/well-known/jwk.json"
            # Internal services
            - name: "TENANT_REGISTRY_URL"
              value: http://release-name-auth-service.default.svc.cluster.local:80
            # Sentry
            - name: SENTRY_ENABLED
              value: "false"
            - name: SENTRY_ENVIRONMENT
              value: ""
            - name: SENTRY_DSN
              value: ""
            - name: SENTRY_TRACES_SAMPLE_RATE
              value: "0"
            - name: SENTRY_PROFILES_SAMPLE_RATE
              value: "0"
            # Mixpanel
            - name: MIXPANEL_ENABLED
              value: "false"
            - name: MIXPANEL_MODE
              value: "proxy"
            - name: MIXPANEL_PROXY_URL
              value: "https://tunnel.monitor.nebuly.com/mixpanel"
            - name: MIXPANEL_USERNAME
              value: ""
            - name: MIXPANEL_PASSWORD
              value: ""
            - name: ANALYTICS_OVERRIDE_TENANT
              value: "release-name"
            # OpenAI
            - name: OPENAI_BASE_URL
              value: "https://nbltstnebuly.openai.azure.com/"
            - name: OPENAI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: nebuly-platform-credentials
                  key: openai-api-key
            - name: OPENAI_DEPLOYMENT_TRANSLATION
              value: ""
            - name: OPENAI_ORGANIZATION
              value: ""
            # Misc
            - name: ENV
              value: "prod"
            - name: SERVER_PORT
              value: "8080"
            - name: PROJECT_NAME
              value: "backend"
            - name: DEVELOPMENT_MODE
              value: "false"
            - name: OPENAPI_URL_ENABLED
              value: "false"
          resources:
            limits:
              memory: 1024Mi
            requests:
              cpu: 100m
          volumeMounts:
            - mountPath: /mnt/secrets-store
              name: secrets-store
              readOnly: true
      containers:
        - name: nebuly-platform
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
          image: "ghcr.io/nebuly-ai/nebuly-backend:v1.57.29"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          env:
            # OTEL
            - name: "OTEL_SERVICE_NAME"
              value: "nebuly-backend"
            # Common env
            
            # Database
            - name: "MULTI_TENANCY_MODE"
              value: "dynamic_schema"
            - name: "ANALYTICS_USER"
              valueFrom:
                secretKeyRef:
                  name: nebuly-platform-credentials
                  key: db-username
            - name: "ANALYTICS_PASSWORD"
              valueFrom:
                secretKeyRef:
                  name: nebuly-platform-credentials
                  key: db-password
            - name: ANALYTICS_DB
              value: "analytics"
            - name: ANALYTICS_SERVER
              value: "nbltstnebulydb3.postgres.database.azure.com"
            # Misc (TODO: remove)
            - name: TENANT
              value: "release-name"
            # OTEL
            - name: OTEL_ENABLED
              value: "false"
            - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
              value: "http://contrib-collector.otel:4317"
            - name: OTEL_EXPORTER_OTLP_METRICS_ENDPOINT
              value: "http://contrib-collector.otel:4317"
            - name: OTEL_METRICS_EXPORTER
              value: "otlp"
            # Old Auth0 stuff, to be removed as soon as we leave Auth0
            - name: AUTH0_ENABLED
              value: "false"
            - name: OAUTH_CLIENT_ID
              value: ""
            - name: OAUTH_CLIENT_SECRET
              value: ""
            - name: OAUTH_DOMAIN
              value: ""
            - name: OAUTH_AUDIENCE
              value: ""
            - name: AUTH0_JWKS_URL
              value: ""
            - name: AUTH0_DATABASE_CONNECTION
              value: ""
            # Oauth
            - name: OAUTH_JWKS_URL
              value: "http://release-name-auth-service:80/auth/well-known/jwk.json"
            # Internal services
            - name: "TENANT_REGISTRY_URL"
              value: http://release-name-auth-service.default.svc.cluster.local:80
            # Sentry
            - name: SENTRY_ENABLED
              value: "false"
            - name: SENTRY_ENVIRONMENT
              value: ""
            - name: SENTRY_DSN
              value: ""
            - name: SENTRY_TRACES_SAMPLE_RATE
              value: "0"
            - name: SENTRY_PROFILES_SAMPLE_RATE
              value: "0"
            # Mixpanel
            - name: MIXPANEL_ENABLED
              value: "false"
            - name: MIXPANEL_MODE
              value: "proxy"
            - name: MIXPANEL_PROXY_URL
              value: "https://tunnel.monitor.nebuly.com/mixpanel"
            - name: MIXPANEL_USERNAME
              value: ""
            - name: MIXPANEL_PASSWORD
              value: ""
            - name: ANALYTICS_OVERRIDE_TENANT
              value: "release-name"
            # OpenAI
            - name: OPENAI_BASE_URL
              value: "https://nbltstnebuly.openai.azure.com/"
            - name: OPENAI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: nebuly-platform-credentials
                  key: openai-api-key
            - name: OPENAI_DEPLOYMENT_TRANSLATION
              value: ""
            - name: OPENAI_ORGANIZATION
              value: ""
            # Misc
            - name: ENV
              value: "prod"
            - name: SERVER_PORT
              value: "8080"
            - name: PROJECT_NAME
              value: "backend"
            - name: DEVELOPMENT_MODE
              value: "false"
            - name: OPENAPI_URL_ENABLED
              value: "false"
          livenessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 10
            periodSeconds: 15
          readinessProbe:
            httpGet:
              path: /readyz
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
          resources:
            limits:
              memory: 1024Mi
            requests:
              cpu: 100m
          volumeMounts:
            - mountPath: /mnt/secrets-store
              name: secrets-store
              readOnly: true
      volumes:
        - csi:
            driver: secrets-store.csi.k8s.io
            readOnly: true
            volumeAttributes:
              secretProviderClass: nebuly-platform
          name: secrets-store
---
# Source: nebuly-platform/templates/backend_scheduler_deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-backend-scheduler
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-backend-scheduler
  annotations:
    nebuly.com/release-name: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: nebuly-platform
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/part-of: nebuly-platform
      app.kubernetes.io/component: nebuly-backend-scheduler
  template:
    metadata:
      labels:
        app.kubernetes.io/name: nebuly-platform
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/part-of: nebuly-platform
        app.kubernetes.io/component: nebuly-backend-scheduler
    spec:
      serviceAccountName: default
      imagePullSecrets:
        - name: nebuly-docker-pull
      securityContext:
        runAsNonRoot: true
      containers:
        - name: nebuly-platform
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
          image: "ghcr.io/nebuly-ai/nebuly-backend:v1.57.29"
          command:
            - python
            - -m
            - scheduling
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          env:
            # OTEL
            - name: "OTEL_SERVICE_NAME"
              value: "nebuly-backend-scheduler"
            - name: "JOB_METRICS_ENABLED"
              value: "true"
            - name: "JOB_USE_NEW_TYPE_OF_RISK_ENABLED"
              value: "false"
            # Common env
            
            # Database
            - name: "MULTI_TENANCY_MODE"
              value: "dynamic_schema"
            - name: "ANALYTICS_USER"
              valueFrom:
                secretKeyRef:
                  name: nebuly-platform-credentials
                  key: db-username
            - name: "ANALYTICS_PASSWORD"
              valueFrom:
                secretKeyRef:
                  name: nebuly-platform-credentials
                  key: db-password
            - name: ANALYTICS_DB
              value: "analytics"
            - name: ANALYTICS_SERVER
              value: "nbltstnebulydb3.postgres.database.azure.com"
            # Misc (TODO: remove)
            - name: TENANT
              value: "release-name"
            # OTEL
            - name: OTEL_ENABLED
              value: "false"
            - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
              value: "http://contrib-collector.otel:4317"
            - name: OTEL_EXPORTER_OTLP_METRICS_ENDPOINT
              value: "http://contrib-collector.otel:4317"
            - name: OTEL_METRICS_EXPORTER
              value: "otlp"
            # Old Auth0 stuff, to be removed as soon as we leave Auth0
            - name: AUTH0_ENABLED
              value: "false"
            - name: OAUTH_CLIENT_ID
              value: ""
            - name: OAUTH_CLIENT_SECRET
              value: ""
            - name: OAUTH_DOMAIN
              value: ""
            - name: OAUTH_AUDIENCE
              value: ""
            - name: AUTH0_JWKS_URL
              value: ""
            - name: AUTH0_DATABASE_CONNECTION
              value: ""
            # Oauth
            - name: OAUTH_JWKS_URL
              value: "http://release-name-auth-service:80/auth/well-known/jwk.json"
            # Internal services
            - name: "TENANT_REGISTRY_URL"
              value: http://release-name-auth-service.default.svc.cluster.local:80
            # Sentry
            - name: SENTRY_ENABLED
              value: "false"
            - name: SENTRY_ENVIRONMENT
              value: ""
            - name: SENTRY_DSN
              value: ""
            - name: SENTRY_TRACES_SAMPLE_RATE
              value: "0"
            - name: SENTRY_PROFILES_SAMPLE_RATE
              value: "0"
            # Mixpanel
            - name: MIXPANEL_ENABLED
              value: "false"
            - name: MIXPANEL_MODE
              value: "proxy"
            - name: MIXPANEL_PROXY_URL
              value: "https://tunnel.monitor.nebuly.com/mixpanel"
            - name: MIXPANEL_USERNAME
              value: ""
            - name: MIXPANEL_PASSWORD
              value: ""
            - name: ANALYTICS_OVERRIDE_TENANT
              value: "release-name"
            # OpenAI
            - name: OPENAI_BASE_URL
              value: "https://nbltstnebuly.openai.azure.com/"
            - name: OPENAI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: nebuly-platform-credentials
                  key: openai-api-key
            - name: OPENAI_DEPLOYMENT_TRANSLATION
              value: ""
            - name: OPENAI_ORGANIZATION
              value: ""
            # Misc
            - name: ENV
              value: "prod"
            - name: SERVER_PORT
              value: "8080"
            - name: PROJECT_NAME
              value: "backend"
            - name: DEVELOPMENT_MODE
              value: "false"
            - name: OPENAPI_URL_ENABLED
              value: "false"
          livenessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 10
            periodSeconds: 15
          readinessProbe:
            httpGet:
              path: /readyz
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
          resources:
            limits:
              memory: 1024Mi
            requests:
              cpu: 100m
          volumeMounts:
            - mountPath: /mnt/secrets-store
              name: secrets-store
              readOnly: true
      volumes:
        - csi:
            driver: secrets-store.csi.k8s.io
            readOnly: true
            volumeAttributes:
              secretProviderClass: nebuly-platform
          name: secrets-store
---
# Source: nebuly-platform/templates/event-ingestion_deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-event-ingestion
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-event-ingestion
  annotations:
    nebuly.com/release-name: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: nebuly-platform
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/part-of: nebuly-platform
      app.kubernetes.io/component: nebuly-event-ingestion
  template:
    metadata:
      labels:
        app.kubernetes.io/name: nebuly-platform
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/part-of: nebuly-platform
        app.kubernetes.io/component: nebuly-event-ingestion
    spec:
      serviceAccountName: default
      imagePullSecrets:
        - name: nebuly-docker-pull
      securityContext:
        runAsNonRoot: true
      containers:
        - name: nebuly-platform
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
          image: "ghcr.io/nebuly-ai/nebuly-event-ingestion:v1.10.0"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          env:
            # Kafka settings
            - name: "KAFKA_BOOTSTRAP_SERVERS"
              value: release-name-kafka-kafka-bootstrap.default.svc:9092
            - name: "KAFKA_SASL_PASSWORD"
              valueFrom:
                  secretKeyRef:
                      name: "nebuly-platform"
                      key: "password"
            - name: "KAFKA_SASL_USERNAME"
              value: "nebuly-platform"
            - name: "KAFKA_SASL_MECHANISM"
              value: "SCRAM-SHA-512"
            - name: "KAFKA_SOCKET_KEEPALIVE_ENABLED"
              value: "true"
            - name: "KAFKA_SSL_CA_PATH"
              value: "/etc/kafka/ca.crt"
            # Kafka topics
            - name: "KAFKA_TOPIC_EVENTS_MAIN"
              value: events-main
            # Platform Services
            - name: "TENANT_REGISTRY_URL"
              value: http://release-name-auth-service.default.svc.cluster.local:80
            # OTEL
            - name: OTEL_ENABLED
              value: "false"
            - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
              value: "http://contrib-collector.otel:4317"
            - name: OTEL_EXPORTER_OTLP_METRICS_ENDPOINT
              value: "http://contrib-collector.otel:4317"
            - name: OTEL_METRICS_EXPORTER
              value: "otlp"
            - name: OTEL_SERVICE_NAME
              value: "nebuly-event-ingestion"
            # Sentry
            - name: SENTRY_ENABLED
              value: "false"
            - name: SENTRY_ENVIRONMENT
              value: ""
            - name: SENTRY_DSN
              value: ""
            - name: SENTRY_TRACES_SAMPLE_RATE
              value: "0"
            - name: SENTRY_PROFILES_SAMPLE_RATE
              value: "0"
            # Misc
            - name: "ENV"
              value: "prod"
            - name: "SERVER_PORT"
              value: "8080"
            - name: "FORWARDED_ALLOW_IPS"
              value: "*"
            - name: "PROJECT_NAME"
              value: "Event Ingestion"
            - name: "DEVELOPMENT_MODE"
              value: "false"
            - name: "ROOT_PATH"
              value: /event-ingestion

          livenessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 10
            periodSeconds: 15
          readinessProbe:
            httpGet:
              path: /readyz
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
          resources:
            limits:
              memory: 256Mi
            requests:
              cpu: 100m
          volumeMounts:
           - name: kafka-cluster-ca-cert
             mountPath: "/etc/kafka"
             readOnly: true
      volumes:
        - name: kafka-cluster-ca-cert
          secret:
            secretName: release-name-kafka-cluster-ca-cert
---
# Source: nebuly-platform/templates/frontend_deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-frontend
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-frontend
  annotations:
    nebuly.com/release-name: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: nebuly-platform
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/part-of: nebuly-platform
      app.kubernetes.io/component: nebuly-frontend
  template:
    metadata:
      annotations:
        checksum/config: 0deab588a7d7841439f6c7d9aba3a9eaf6ce12d5b945a6dc7c0a6b5d3d79314a
      labels:
        app.kubernetes.io/name: nebuly-platform
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/part-of: nebuly-platform
        app.kubernetes.io/component: nebuly-frontend
    spec:
      serviceAccountName: default
      imagePullSecrets:
        - name: nebuly-docker-pull
      securityContext:
        runAsNonRoot: true

      initContainers:
        - name: populate-env-json
          image: "ghcr.io/nebuly-ai/nebuly-frontend:v1.50.38"
          command: [ "/bin/sh", "-c" ]
          args:
            - sed "s/\${GIT_REF}/$GIT_REF/g" /env.json > /output/env.json
          volumeMounts:
            - name: config
              mountPath: /env.json
              subPath: env.json
            - name: shared
              mountPath: /output


      containers:
        - name: nebuly-platform
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
          image: "ghcr.io/nebuly-ai/nebuly-frontend:v1.50.38"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP

          livenessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 3
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /env.json
              port: http
            initialDelaySeconds: 5
            periodSeconds: 5

          resources:
            limits:
              memory: 128Mi
            requests:
              cpu: 100m
          volumeMounts:
            - name: shared
              mountPath: /app/public/env.json
              subPath: env.json
      volumes:
        - name: "config"
          configMap:
            name: release-name-frontend
        - name: "shared"
          emptyDir: { }
---
# Source: nebuly-platform/templates/ingestion-worker_deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-ingestion-worker
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-ingestion-worker
  annotations:
    nebuly.com/release-name: release-name
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: nebuly-platform
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/part-of: nebuly-platform
      app.kubernetes.io/component: nebuly-ingestion-worker
  template:
    metadata:
      labels:
        app.kubernetes.io/name: nebuly-platform
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/part-of: nebuly-platform
        app.kubernetes.io/component: nebuly-ingestion-worker
    spec:
      serviceAccountName: default
      imagePullSecrets:
        - name: nebuly-docker-pull
      securityContext:
        runAsNonRoot: true
      containers:
        # Container 1
        - name: "nebuly-platform-1"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
          command:
            - python
            - entrypoints/entrypoint_1.py
          image: "ghcr.io/nebuly-ai/nebuly-ingestion-worker:v1.47.28"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          env:
            # Misc
            - name: ENV
              value: "prod"
            - name: DEVELOPMENT_MODE
              value: "false"
            - name: TOUCH_EVERY_SECONDS
              value: "10"
            # Workers
            - name: NUMBER_OF_WORKERS_ACTIONS
              value: "10"
            - name: NUMBER_OF_WORKERS_INTERACTIONS
              value: "20"
            - name: NUMBER_OF_WORKERS_FEEDBACK_ACTIONS
              value: "10"
            # Misc (TODO: remove)
            - name: TENANT
              value: "release-name"
            # OTEL
            - name: OTEL_SERVICE_NAME
              value: "nebuly-ingestion-worker"
            - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
              value: "http://contrib-collector.otel:4317"
            - name: OTEL_EXPORTER_OTLP_METRICS_ENDPOINT
              value: "http://contrib-collector.otel:4317"
            - name: OTEL_ENABLED
              value: "false"
            - name: OTEL_METRICS_EXPORTER
              value: "otlp"
            # PostgreSQL
            - name: POSTGRES_DB
              value: "analytics"
            - name: POSTGRES_SERVER
              value: "nbltstnebulydb3.postgres.database.azure.com"
            - name: "POSTGRES_USER"
              valueFrom:
                secretKeyRef:
                  name: nebuly-platform-credentials
                  key: db-username
            - name: "POSTGRES_PASSWORD"
              valueFrom:
                secretKeyRef:
                  name: nebuly-platform-credentials
                  key: db-password
            - name: "STATEMENT_TIMEOUT_SECONDS"
              value: "120"
            # Kafka Settings
            - name: KAFKA_SOCKET_KEEPALIVE_ENABLED
              value: "true"
            - name: KAFKA_BOOTSTRAP_SERVERS
              value: release-name-kafka-kafka-bootstrap.default.svc:9092
            - name: KAFKA_TOPIC_EVENTS_MAIN
              value: "events-main"
            - name: KAFKA_TOPIC_EVENTS_RETRY_1
              value: "events-retry-1"
            - name: KAFKA_TOPIC_EVENTS_RETRY_2
              value: "events-retry-2"
            - name: KAFKA_TOPIC_EVENTS_RETRY_3
              value: "events-retry-3"
            - name: KAFKA_TOPIC_EVENTS_DLQ
              value: "events-dlq"
            - name: "KAFKA_SASL_MECHANISM"
              value: "SCRAM-SHA-512"
            - name: "KAFKA_SASL_PASSWORD"
              valueFrom:
                secretKeyRef:
                    name: "nebuly-platform"
                    key: "password"
            - name: "KAFKA_SASL_USERNAME"
              value: "nebuly-platform"
            - name: "KAFKA_SSL_CA_PATH"
              value: "/etc/kafka/ca.crt"
            # Platform services
            - name: "TENANT_REGISTRY_URL"
              value: http://release-name-auth-service.default.svc.cluster.local:80
            - name: BACKEND_URL
              value: http://release-name-backend.default.svc.cluster.local:80
            # Sentry
            - name: SENTRY_ENABLED
              value: "false"
            - name: SENTRY_ENVIRONMENT
              value: ""
            - name: SENTRY_DSN
              value: ""
            - name: SENTRY_TRACES_SAMPLE_RATE
              value: "0"
            - name: SENTRY_PROFILES_SAMPLE_RATE
              value: "0"
          livenessProbe:
            exec:
              command:
                - python
                - app/health_check.py
                - "60" # max allowed stale seconds
            initialDelaySeconds: 15
            periodSeconds: 10
          resources:
            limits:
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 1024Mi
          volumeMounts:
            - name: kafka-cluster-ca-cert
              mountPath: "/etc/kafka"
              readOnly: true
            - mountPath: /mnt/secrets-store
              name: secrets-store
              readOnly: true

        # Container 2
        - name: "nebuly-platform-2"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
          command:
            - python
            - entrypoints/entrypoint_2.py
          image: "ghcr.io/nebuly-ai/nebuly-ingestion-worker:v1.47.28"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          env:
            # Misc
            - name: ENV
              value: "prod"
            - name: DEVELOPMENT_MODE
              value: "false"
            - name: TOUCH_EVERY_SECONDS
              value: "10"
            # Workers
            - name: NUMBER_OF_WORKERS_ACTIONS
              value: "10"
            - name: NUMBER_OF_WORKERS_INTERACTIONS
              value: "20"
            - name: NUMBER_OF_WORKERS_FEEDBACK_ACTIONS
              value: "10"
            # Misc (TODO: remove)
            - name: TENANT
              value: "release-name"
            # OTEL
            - name: OTEL_SERVICE_NAME
              value: "nebuly-ingestion-worker"
            - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
              value: "http://contrib-collector.otel:4317"
            - name: OTEL_EXPORTER_OTLP_METRICS_ENDPOINT
              value: "http://contrib-collector.otel:4317"
            - name: OTEL_ENABLED
              value: "false"
            - name: OTEL_METRICS_EXPORTER
              value: "otlp"
            # PostgreSQL
            - name: POSTGRES_DB
              value: "analytics"
            - name: POSTGRES_SERVER
              value: "nbltstnebulydb3.postgres.database.azure.com"
            - name: "POSTGRES_USER"
              valueFrom:
                secretKeyRef:
                  name: nebuly-platform-credentials
                  key: db-username
            - name: "POSTGRES_PASSWORD"
              valueFrom:
                secretKeyRef:
                  name: nebuly-platform-credentials
                  key: db-password
            - name: "STATEMENT_TIMEOUT_SECONDS"
              value: "120"
            # Kafka Settings
            - name: KAFKA_SOCKET_KEEPALIVE_ENABLED
              value: "true"
            - name: KAFKA_BOOTSTRAP_SERVERS
              value: release-name-kafka-kafka-bootstrap.default.svc:9092
            - name: KAFKA_TOPIC_EVENTS_MAIN
              value: "events-main"
            - name: KAFKA_TOPIC_EVENTS_RETRY_1
              value: "events-retry-1"
            - name: KAFKA_TOPIC_EVENTS_RETRY_2
              value: "events-retry-2"
            - name: KAFKA_TOPIC_EVENTS_RETRY_3
              value: "events-retry-3"
            - name: KAFKA_TOPIC_EVENTS_DLQ
              value: "events-dlq"
            - name: "KAFKA_SASL_MECHANISM"
              value: "SCRAM-SHA-512"
            - name: "KAFKA_SASL_PASSWORD"
              valueFrom:
                secretKeyRef:
                    name: "nebuly-platform"
                    key: "password"
            - name: "KAFKA_SASL_USERNAME"
              value: "nebuly-platform"
            - name: "KAFKA_SSL_CA_PATH"
              value: "/etc/kafka/ca.crt"
            # Platform services
            - name: "TENANT_REGISTRY_URL"
              value: http://release-name-auth-service.default.svc.cluster.local:80
            - name: BACKEND_URL
              value: http://release-name-backend.default.svc.cluster.local:80
            # Sentry
            - name: SENTRY_ENABLED
              value: "false"
            - name: SENTRY_ENVIRONMENT
              value: ""
            - name: SENTRY_DSN
              value: ""
            - name: SENTRY_TRACES_SAMPLE_RATE
              value: "0"
            - name: SENTRY_PROFILES_SAMPLE_RATE
              value: "0"
          livenessProbe:
            exec:
              command:
                - python
                - app/health_check.py
                - "60" # max allowed stale seconds
            initialDelaySeconds: 15
            periodSeconds: 10
          resources:
            limits:
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 1024Mi
          volumeMounts:
            - name: kafka-cluster-ca-cert
              mountPath: "/etc/kafka"
              readOnly: true
            - mountPath: /mnt/secrets-store
              name: secrets-store
              readOnly: true

      volumes:
        - name: kafka-cluster-ca-cert
          secret:
            secretName: release-name-kafka-cluster-ca-cert
        - csi:
            driver: secrets-store.csi.k8s.io
            readOnly: true
            volumeAttributes:
              secretProviderClass: nebuly-platform
          name: secrets-store
---
# Source: nebuly-platform/templates/primary-enrich-interactions_cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: release-name-enrich-interactions
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    platform.nebuly.com/processing-stage: primary
    app.kubernetes.io/component: job-enrich-interactions
  annotations:
    nebuly.com/release-name: release-name
spec:
  schedule: "0 23 * * *"
  jobTemplate:
    metadata:
      labels:
          app.kubernetes.io/name: nebuly-platform
          app.kubernetes.io/instance: release-name
          app.kubernetes.io/part-of: nebuly-platform
          platform.nebuly.com/processing-stage: primary
          app.kubernetes.io/component: job-enrich-interactions
    spec:
      backoffLimit: 1
      activeDeadlineSeconds: 10800
      template:
        metadata:
          labels:
              app.kubernetes.io/name: nebuly-platform
              app.kubernetes.io/instance: release-name
              app.kubernetes.io/part-of: nebuly-platform
              platform.nebuly.com/processing-stage: primary
              app.kubernetes.io/component: job-enrich-interactions
        spec:
          hostIPC: false
          serviceAccountName: default
          restartPolicy: Never
          imagePullSecrets:
              - name: nebuly-docker-pull
          securityContext:
            fsGroup: 101
            runAsNonRoot: true
          containers:
            - name: nebuly-platform
              securityContext:
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                  - ALL
                runAsNonRoot: true
              image: "ghcr.io/nebuly-ai/nebuly-inference:v1.47.28"
              command: [ "python", "jobs/process_enrich_interactions.py" ]
              imagePullPolicy: IfNotPresent
              env:
              
                # Misc
                - name: ENV
                  value: "prod"
                - name: DEVELOPMENT_MODE
                  value: "false"
                - name: TOUCH_EVERY_SECONDS
                  value: "10"
                # Workers
                - name: NUMBER_OF_WORKERS_ACTIONS
                  value: "10"
                - name: NUMBER_OF_WORKERS_INTERACTIONS
                  value: "20"
                - name: NUMBER_OF_WORKERS_FEEDBACK_ACTIONS
                  value: "10"
                # Misc (TODO: remove)
                - name: TENANT
                  value: "release-name"
                # OTEL
                - name: OTEL_SERVICE_NAME
                  value: "nebuly-ingestion-worker"
                - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
                  value: "http://contrib-collector.otel:4317"
                - name: OTEL_EXPORTER_OTLP_METRICS_ENDPOINT
                  value: "http://contrib-collector.otel:4317"
                - name: OTEL_ENABLED
                  value: "false"
                - name: OTEL_METRICS_EXPORTER
                  value: "otlp"
                # PostgreSQL
                - name: POSTGRES_DB
                  value: "analytics"
                - name: POSTGRES_SERVER
                  value: "nbltstnebulydb3.postgres.database.azure.com"
                - name: "POSTGRES_USER"
                  valueFrom:
                    secretKeyRef:
                      name: nebuly-platform-credentials
                      key: db-username
                - name: "POSTGRES_PASSWORD"
                  valueFrom:
                    secretKeyRef:
                      name: nebuly-platform-credentials
                      key: db-password
                - name: "STATEMENT_TIMEOUT_SECONDS"
                  value: "120"
                # Kafka Settings
                - name: KAFKA_SOCKET_KEEPALIVE_ENABLED
                  value: "true"
                - name: KAFKA_BOOTSTRAP_SERVERS
                  value: release-name-kafka-kafka-bootstrap.default.svc:9092
                - name: KAFKA_TOPIC_EVENTS_MAIN
                  value: "events-main"
                - name: KAFKA_TOPIC_EVENTS_RETRY_1
                  value: "events-retry-1"
                - name: KAFKA_TOPIC_EVENTS_RETRY_2
                  value: "events-retry-2"
                - name: KAFKA_TOPIC_EVENTS_RETRY_3
                  value: "events-retry-3"
                - name: KAFKA_TOPIC_EVENTS_DLQ
                  value: "events-dlq"
                - name: "KAFKA_SASL_MECHANISM"
                  value: "SCRAM-SHA-512"
                - name: "KAFKA_SASL_PASSWORD"
                  valueFrom:
                    secretKeyRef:
                        name: "nebuly-platform"
                        key: "password"
                - name: "KAFKA_SASL_USERNAME"
                  value: "nebuly-platform"
                - name: "KAFKA_SSL_CA_PATH"
                  value: "/etc/kafka/ca.crt"
                # Platform services
                - name: "TENANT_REGISTRY_URL"
                  value: http://release-name-auth-service.default.svc.cluster.local:80
                - name: BACKEND_URL
                  value: http://release-name-backend.default.svc.cluster.local:80
                # Sentry
                - name: SENTRY_ENABLED
                  value: "false"
                - name: SENTRY_ENVIRONMENT
                  value: ""
                - name: SENTRY_DSN
                  value: ""
                - name: SENTRY_TRACES_SAMPLE_RATE
                  value: "0"
                - name: SENTRY_PROFILES_SAMPLE_RATE
                  value: "0"
              
                # Enrich interaction Model
                - name: MODEL_NAME
                  value: "interaction-analyzer-7b-v2"
                - name: MODEL_VERSION
                  value: "20"
                
                # Topic Model
                - name: TOPIC_LOCAL_MODEL_NAME
                  value: "topic-classifier"
                - name: TOPIC_LOCAL_MODEL_VERSION
                  value: "6"
                
                # Action Model
                - name: ACTION_LOCAL_MODEL_NAME
                  value: "action-classifier"
                - name: ACTION_LOCAL_MODEL_VERSION
                  value: "6"
                
                # Jobs Settings
                - name: ENABLE_DB_CACHE
                  value: true
                - name: ENTITIES_BATCH_SIZE
                  value: 20000
                - name: ENRICH_INTERACTION_BATCH_SIZE
                  value: 10000
                - name: LOOP_JOBS_SLEEP_SECONDS
                  value: 0

                # OpenAI
                - name: OPENAI_API_VERSION
                  value: "2024-02-15-preview"
                - name: OPENAI_BASE_URL
                  value: "https://nbltstnebuly.openai.azure.com/"
                - name: OPENAI_DEPLOYMENT_FRUSTRATION
                  value: "nbltst-gpt-4o"
                - name: OPENAI_DEPLOYMENT_GPT4O
                  value: "nbltst-gpt-4o"
                - name: OPENAI_API_KEY
                  valueFrom:
                    secretKeyRef:
                      name: nebuly-platform-credentials
                      key: openai-api-key
                
                # Models
                - name: MODEL_PROVIDER
                  value: "azure_storage"
                - name: MODELS_CACHE_DIR
                  value: "/var/cache/nebuly"
                - name: SENTENCE_TRANSFORMERS_HOME
                  value: "/tmp/hf"
                - name: HF_HOME
                  value: "/tmp/hf"
                - name: AZURE_STORAGE_ACCOUNT_NAME
                  value: "nbltstmodels"
                - name: AZURE_STORAGE_CONTAINER_NAME
                  value: "models"
                - name: "AZURE_TENANT_ID"
                  value: "4e16af92-dd38-417c-bf9d-a24f891a7987"
                - name: "AZURE_CLIENT_ID"
                  value: "091ca61a-1218-4ac5-94f4-02c7400d08d0"
              resources:
                  limits:
                    nvidia.com/gpu: 1
                  requests:
                    cpu: 1
              volumeMounts:
                - name: vllm-report-usage
                  mountPath: /nonexistent
                - name: kafka-cluster-ca-cert
                  mountPath: "/etc/kafka"
                  readOnly: true
                - name: models-cache
                  mountPath: /var/cache/nebuly
                  readOnly: false
          volumes:
            - name: vllm-report-usage
              emptyDir: {}
            - name: kafka-cluster-ca-cert
              secret:
                secretName: release-name-kafka-cluster-ca-cert
            - name: models-cache
              emptyDir: { }
          affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: nebuly.com/accelerator
                      operator: In
                      values:
                      - nvidia-ampere-a100
          tolerations:
              - effect: NoSchedule
                key: nvidia.com/gpu
                operator: Exists
---
# Source: nebuly-platform/templates/secondary-process-model-suggestions_cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: release-name-process-model-suggestions
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    platform.nebuly.com/processing-stage: secondary
    app.kubernetes.io/component: job-process-model-suggestions
  annotations:
    nebuly.com/release-name: release-name
spec:
  schedule: "0 2 * * *"
  jobTemplate:
    metadata:
      labels:
          app.kubernetes.io/name: nebuly-platform
          app.kubernetes.io/instance: release-name
          app.kubernetes.io/part-of: nebuly-platform
          platform.nebuly.com/processing-stage: secondary
          app.kubernetes.io/component: job-process-model-suggestions
    spec:
      backoffLimit: 1
      activeDeadlineSeconds: 10800
      template:
        metadata:
          labels:
              app.kubernetes.io/name: nebuly-platform
              app.kubernetes.io/instance: release-name
              app.kubernetes.io/part-of: nebuly-platform
              platform.nebuly.com/processing-stage: secondary
              app.kubernetes.io/component: job-process-model-suggestions
        spec:
          hostIPC: false
          serviceAccountName: default
          restartPolicy: Never
          imagePullSecrets:
              - name: nebuly-docker-pull
          securityContext:
            fsGroup: 101
            runAsNonRoot: true
          containers:
            - name: nebuly-platform
              securityContext:
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                  - ALL
                runAsNonRoot: true
              image: "ghcr.io/nebuly-ai/nebuly-inference:v1.47.28"
              command: [ "python", "jobs/process_model_suggestions.py" ]
              imagePullPolicy: IfNotPresent
              env:
              
                # Misc
                - name: ENV
                  value: "prod"
                - name: DEVELOPMENT_MODE
                  value: "false"
                - name: TOUCH_EVERY_SECONDS
                  value: "10"
                # Workers
                - name: NUMBER_OF_WORKERS_ACTIONS
                  value: "10"
                - name: NUMBER_OF_WORKERS_INTERACTIONS
                  value: "20"
                - name: NUMBER_OF_WORKERS_FEEDBACK_ACTIONS
                  value: "10"
                # Misc (TODO: remove)
                - name: TENANT
                  value: "release-name"
                # OTEL
                - name: OTEL_SERVICE_NAME
                  value: "nebuly-ingestion-worker"
                - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
                  value: "http://contrib-collector.otel:4317"
                - name: OTEL_EXPORTER_OTLP_METRICS_ENDPOINT
                  value: "http://contrib-collector.otel:4317"
                - name: OTEL_ENABLED
                  value: "false"
                - name: OTEL_METRICS_EXPORTER
                  value: "otlp"
                # PostgreSQL
                - name: POSTGRES_DB
                  value: "analytics"
                - name: POSTGRES_SERVER
                  value: "nbltstnebulydb3.postgres.database.azure.com"
                - name: "POSTGRES_USER"
                  valueFrom:
                    secretKeyRef:
                      name: nebuly-platform-credentials
                      key: db-username
                - name: "POSTGRES_PASSWORD"
                  valueFrom:
                    secretKeyRef:
                      name: nebuly-platform-credentials
                      key: db-password
                - name: "STATEMENT_TIMEOUT_SECONDS"
                  value: "120"
                # Kafka Settings
                - name: KAFKA_SOCKET_KEEPALIVE_ENABLED
                  value: "true"
                - name: KAFKA_BOOTSTRAP_SERVERS
                  value: release-name-kafka-kafka-bootstrap.default.svc:9092
                - name: KAFKA_TOPIC_EVENTS_MAIN
                  value: "events-main"
                - name: KAFKA_TOPIC_EVENTS_RETRY_1
                  value: "events-retry-1"
                - name: KAFKA_TOPIC_EVENTS_RETRY_2
                  value: "events-retry-2"
                - name: KAFKA_TOPIC_EVENTS_RETRY_3
                  value: "events-retry-3"
                - name: KAFKA_TOPIC_EVENTS_DLQ
                  value: "events-dlq"
                - name: "KAFKA_SASL_MECHANISM"
                  value: "SCRAM-SHA-512"
                - name: "KAFKA_SASL_PASSWORD"
                  valueFrom:
                    secretKeyRef:
                        name: "nebuly-platform"
                        key: "password"
                - name: "KAFKA_SASL_USERNAME"
                  value: "nebuly-platform"
                - name: "KAFKA_SSL_CA_PATH"
                  value: "/etc/kafka/ca.crt"
                # Platform services
                - name: "TENANT_REGISTRY_URL"
                  value: http://release-name-auth-service.default.svc.cluster.local:80
                - name: BACKEND_URL
                  value: http://release-name-backend.default.svc.cluster.local:80
                # Sentry
                - name: SENTRY_ENABLED
                  value: "false"
                - name: SENTRY_ENVIRONMENT
                  value: ""
                - name: SENTRY_DSN
                  value: ""
                - name: SENTRY_TRACES_SAMPLE_RATE
                  value: "0"
                - name: SENTRY_PROFILES_SAMPLE_RATE
                  value: "0"
              
                # Enrich interaction Model
                - name: MODEL_NAME
                  value: "interaction-analyzer-7b-v2"
                - name: MODEL_VERSION
                  value: "20"
                
                # Topic Model
                - name: TOPIC_LOCAL_MODEL_NAME
                  value: "topic-classifier"
                - name: TOPIC_LOCAL_MODEL_VERSION
                  value: "6"
                
                # Action Model
                - name: ACTION_LOCAL_MODEL_NAME
                  value: "action-classifier"
                - name: ACTION_LOCAL_MODEL_VERSION
                  value: "6"
                
                # Jobs Settings
                - name: ENABLE_DB_CACHE
                  value: true
                - name: ENTITIES_BATCH_SIZE
                  value: 20000
                - name: ENRICH_INTERACTION_BATCH_SIZE
                  value: 10000
                - name: LOOP_JOBS_SLEEP_SECONDS
                  value: 0
                
                
                # OpenAI
                - name: OPENAI_API_VERSION
                  value: "2024-02-15-preview"
                - name: OPENAI_BASE_URL
                  value: "https://nbltstnebuly.openai.azure.com/"
                - name: OPENAI_DEPLOYMENT_FRUSTRATION
                  value: "nbltst-gpt-4o"
                - name: OPENAI_DEPLOYMENT_GPT4O
                  value: "nbltst-gpt-4o"
                - name: OPENAI_API_KEY
                  valueFrom:
                    secretKeyRef:
                      name: nebuly-platform-credentials
                      key: openai-api-key
                
                # Models
                - name: MODEL_PROVIDER
                  value: "azure_storage"
                - name: MODELS_CACHE_DIR
                  value: "/var/cache/nebuly"
                - name: SENTENCE_TRANSFORMERS_HOME
                  value: "/tmp/hf"
                - name: HF_HOME
                  value: "/tmp/hf"
                - name: AZURE_STORAGE_ACCOUNT_NAME
                  value: "nbltstmodels"
                - name: AZURE_STORAGE_CONTAINER_NAME
                  value: "models"
                - name: "AZURE_TENANT_ID"
                  value: "4e16af92-dd38-417c-bf9d-a24f891a7987"
                - name: "AZURE_CLIENT_ID"
                  value: "091ca61a-1218-4ac5-94f4-02c7400d08d0"
              resources:
                  limits:
                    nvidia.com/gpu: 1
                  requests:
                    cpu: 1
              volumeMounts:
                - name: vllm-report-usage
                  mountPath: /nonexistent
                - name: kafka-cluster-ca-cert
                  mountPath: "/etc/kafka"
                  readOnly: true
                - name: models-cache
                  mountPath: /var/cache/nebuly
                  readOnly: false
          volumes:
            - name: vllm-report-usage
              emptyDir: {}
            - name: kafka-cluster-ca-cert
              secret:
                secretName: release-name-kafka-cluster-ca-cert
            - name: models-cache
              emptyDir: { }
          affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: nebuly.com/accelerator
                      operator: In
                      values:
                      - nvidia-ampere-a100
          tolerations:
              - effect: NoSchedule
                key: nvidia.com/gpu
                operator: Exists
---
# Source: nebuly-platform/templates/secondary-topics-and-actions_cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: release-name-process-topics-and-actions
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    platform.nebuly.com/processing-stage: secondary
    app.kubernetes.io/component: job-process-topics-and-actions
  annotations:
    nebuly.com/release-name: release-name
spec:
  schedule: "0 2 * * *"
  jobTemplate:
    metadata:
      labels:
          app.kubernetes.io/name: nebuly-platform
          app.kubernetes.io/instance: release-name
          app.kubernetes.io/part-of: nebuly-platform
          platform.nebuly.com/processing-stage: secondary
          app.kubernetes.io/component: job-process-topics-and-actions
    spec:
      backoffLimit: 0
      activeDeadlineSeconds: 10800
      template:
        metadata:
          labels:
              app.kubernetes.io/name: nebuly-platform
              app.kubernetes.io/instance: release-name
              app.kubernetes.io/part-of: nebuly-platform
              platform.nebuly.com/processing-stage: secondary
              app.kubernetes.io/component: job-process-topics-and-actions
        spec:
          hostIPC: false
          serviceAccountName: default
          restartPolicy: Never
          imagePullSecrets:
              - name: nebuly-docker-pull
          securityContext:
            fsGroup: 101
            runAsNonRoot: true
          containers:
            - name: nebuly-platform
              securityContext:
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                  - ALL
                runAsNonRoot: true
              image: "ghcr.io/nebuly-ai/nebuly-inference:v1.47.28"
              command: [ "python", "jobs/process_topics_and_actions.py" ]
              imagePullPolicy: IfNotPresent
              env:
                
                # Misc
                - name: ENV
                  value: "prod"
                - name: DEVELOPMENT_MODE
                  value: "false"
                - name: TOUCH_EVERY_SECONDS
                  value: "10"
                # Workers
                - name: NUMBER_OF_WORKERS_ACTIONS
                  value: "10"
                - name: NUMBER_OF_WORKERS_INTERACTIONS
                  value: "20"
                - name: NUMBER_OF_WORKERS_FEEDBACK_ACTIONS
                  value: "10"
                # Misc (TODO: remove)
                - name: TENANT
                  value: "release-name"
                # OTEL
                - name: OTEL_SERVICE_NAME
                  value: "nebuly-ingestion-worker"
                - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
                  value: "http://contrib-collector.otel:4317"
                - name: OTEL_EXPORTER_OTLP_METRICS_ENDPOINT
                  value: "http://contrib-collector.otel:4317"
                - name: OTEL_ENABLED
                  value: "false"
                - name: OTEL_METRICS_EXPORTER
                  value: "otlp"
                # PostgreSQL
                - name: POSTGRES_DB
                  value: "analytics"
                - name: POSTGRES_SERVER
                  value: "nbltstnebulydb3.postgres.database.azure.com"
                - name: "POSTGRES_USER"
                  valueFrom:
                    secretKeyRef:
                      name: nebuly-platform-credentials
                      key: db-username
                - name: "POSTGRES_PASSWORD"
                  valueFrom:
                    secretKeyRef:
                      name: nebuly-platform-credentials
                      key: db-password
                - name: "STATEMENT_TIMEOUT_SECONDS"
                  value: "120"
                # Kafka Settings
                - name: KAFKA_SOCKET_KEEPALIVE_ENABLED
                  value: "true"
                - name: KAFKA_BOOTSTRAP_SERVERS
                  value: release-name-kafka-kafka-bootstrap.default.svc:9092
                - name: KAFKA_TOPIC_EVENTS_MAIN
                  value: "events-main"
                - name: KAFKA_TOPIC_EVENTS_RETRY_1
                  value: "events-retry-1"
                - name: KAFKA_TOPIC_EVENTS_RETRY_2
                  value: "events-retry-2"
                - name: KAFKA_TOPIC_EVENTS_RETRY_3
                  value: "events-retry-3"
                - name: KAFKA_TOPIC_EVENTS_DLQ
                  value: "events-dlq"
                - name: "KAFKA_SASL_MECHANISM"
                  value: "SCRAM-SHA-512"
                - name: "KAFKA_SASL_PASSWORD"
                  valueFrom:
                    secretKeyRef:
                        name: "nebuly-platform"
                        key: "password"
                - name: "KAFKA_SASL_USERNAME"
                  value: "nebuly-platform"
                - name: "KAFKA_SSL_CA_PATH"
                  value: "/etc/kafka/ca.crt"
                # Platform services
                - name: "TENANT_REGISTRY_URL"
                  value: http://release-name-auth-service.default.svc.cluster.local:80
                - name: BACKEND_URL
                  value: http://release-name-backend.default.svc.cluster.local:80
                # Sentry
                - name: SENTRY_ENABLED
                  value: "false"
                - name: SENTRY_ENVIRONMENT
                  value: ""
                - name: SENTRY_DSN
                  value: ""
                - name: SENTRY_TRACES_SAMPLE_RATE
                  value: "0"
                - name: SENTRY_PROFILES_SAMPLE_RATE
                  value: "0"
                
                # Enrich interaction Model
                - name: MODEL_NAME
                  value: "interaction-analyzer-7b-v2"
                - name: MODEL_VERSION
                  value: "20"
                
                # Topic Model
                - name: TOPIC_LOCAL_MODEL_NAME
                  value: "topic-classifier"
                - name: TOPIC_LOCAL_MODEL_VERSION
                  value: "6"
                
                # Action Model
                - name: ACTION_LOCAL_MODEL_NAME
                  value: "action-classifier"
                - name: ACTION_LOCAL_MODEL_VERSION
                  value: "6"
                
                # Jobs Settings
                - name: ENABLE_DB_CACHE
                  value: true
                - name: ENTITIES_BATCH_SIZE
                  value: 20000
                - name: ENRICH_INTERACTION_BATCH_SIZE
                  value: 10000
                - name: LOOP_JOBS_SLEEP_SECONDS
                  value: 0
                
                
                # OpenAI
                - name: OPENAI_API_VERSION
                  value: "2024-02-15-preview"
                - name: OPENAI_BASE_URL
                  value: "https://nbltstnebuly.openai.azure.com/"
                - name: OPENAI_DEPLOYMENT_FRUSTRATION
                  value: "nbltst-gpt-4o"
                - name: OPENAI_DEPLOYMENT_GPT4O
                  value: "nbltst-gpt-4o"
                - name: OPENAI_API_KEY
                  valueFrom:
                    secretKeyRef:
                      name: nebuly-platform-credentials
                      key: openai-api-key
                
                # Models
                - name: MODEL_PROVIDER
                  value: "azure_storage"
                - name: MODELS_CACHE_DIR
                  value: "/var/cache/nebuly"
                - name: SENTENCE_TRANSFORMERS_HOME
                  value: "/tmp/hf"
                - name: HF_HOME
                  value: "/tmp/hf"
                - name: AZURE_STORAGE_ACCOUNT_NAME
                  value: "nbltstmodels"
                - name: AZURE_STORAGE_CONTAINER_NAME
                  value: "models"
                - name: "AZURE_TENANT_ID"
                  value: "4e16af92-dd38-417c-bf9d-a24f891a7987"
                - name: "AZURE_CLIENT_ID"
                  value: "091ca61a-1218-4ac5-94f4-02c7400d08d0"
              resources:
                limits:
                  nvidia.com/gpu: 1
                requests:
                  cpu: 1
              volumeMounts:
                - name: vllm-report-usage
                  mountPath: /nonexistent
                - name: models-cache
                  mountPath: /var/cache/nebuly
          volumes:
            - name: vllm-report-usage
              emptyDir: {}
            - name: models-cache
              emptyDir: { }
          affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: nebuly.com/accelerator
                      operator: In
                      values:
                      - nvidia-ampere-a100
          tolerations:
              - effect: NoSchedule
                key: nvidia.com/gpu
                operator: Exists
---
# Source: nebuly-platform/templates/auth-service_ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: release-name-auth-service
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-auth-service
  annotations:
    nebuly.com/release-name: release-name
    cert-manager.io/cluster-issuer: letsencrypt
    nginx.ingress.kubernetes.io/rewrite-target: /auth/$2
    nginx.ingress.kubernetes.io/use-regex: "true"
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - "platform.azure.testing.nebuly.com"
      secretName: nebuly-tls
  rules:
    - host: "platform.azure.testing.nebuly.com"
      http:
        paths:
          - path: /backend/auth(/|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: release-name-auth-service
                port:
                  number: 80
---
# Source: nebuly-platform/templates/backend_ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: release-name-backend
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-backend
  annotations:
    nebuly.com/release-name: release-name
    cert-manager.io/cluster-issuer: letsencrypt
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - "platform.azure.testing.nebuly.com"
      secretName: nebuly-tls
  rules:
    - host: "platform.azure.testing.nebuly.com"
      http:
        paths:
          - path: /backend(/|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: release-name-backend
                port:
                  number: 80
---
# Source: nebuly-platform/templates/event-ingestion_ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: release-name-event-ingestion
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-event-ingestion
  annotations:
    nebuly.com/release-name: release-name
    cert-manager.io/cluster-issuer: letsencrypt
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - "platform.azure.testing.nebuly.com"
      secretName: nebuly-tls
  rules:
    - host: "platform.azure.testing.nebuly.com"
      http:
        paths:
          - path: /event-ingestion(/|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: release-name-event-ingestion
                port:
                  number: 80
---
# Source: nebuly-platform/templates/frontend_ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: release-name-frontend
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: nebuly-frontend
  annotations:
    nebuly.com/release-name: release-name
    cert-manager.io/cluster-issuer: letsencrypt
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - "platform.azure.testing.nebuly.com"
      secretName: nebuly-tls
  rules:
    - host: "platform.azure.testing.nebuly.com"
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: release-name-frontend
                port:
                  number: 80
---
# Source: nebuly-platform/templates/letsencrypt.cluster-issuer.yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt
spec:
  acme:
    email: support@nebuly.ai
    privateKeySecretRef:
      name: letsencrypt
    server: https://acme-v02.api.letsencrypt.org/directory
    solvers:
    - http01:
        ingress:
          class: nginx
          podTemplate:
            spec:
              nodeSelector:
                "kubernetes.io/os": linux
---
# Source: nebuly-platform/templates/kafka-cluster_main.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: release-name-kafka
  namespace: default
  annotations:
    nebuly.com/release-name: release-name
spec:
  zookeeper:
    template:
      pod:
        affinity:
          {}
    replicas: 1
    storage:
      class: managed-csi-zrs
      deleteClaim: false
      size: 10Gi
      type: persistent-claim
    resources:
      limits:
        memory: 2048Mi
      requests:
        cpu: 100m
        memory: 1024Mi
  entityOperator:
    userOperator: {}
    topicOperator: {}
  kafka:
    template:
      pod:
        affinity:
          {}
    replicas: 1
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: true
        authentication:
          type: scram-sha-512
    authorization:
      type: simple
    storage:
      class: managed-csi-zrs
      deleteClaim: false
      size: 32Gi
      type: persistent-claim
    resources:
      limits:
        memory: 6Gi
      requests:
        cpu: 100m
        memory: 6Gi
    rack:
      topologyKey: topology.kubernetes.io/zone
    config:
      offsets.topic.replication.factor: 1
      replica.selector.class: org.apache.kafka.common.replica.RackAwareReplicaSelector
---
# Source: nebuly-platform/templates/kafka-topic_events-main.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: events-main
  namespace: default
  labels:
    strimzi.io/cluster: release-name-kafka
  annotations:
    nebuly.com/release-name: release-name
spec:
  topicName: events-main
  partitions: 8
  replicas: 1
---
# Source: nebuly-platform/templates/kafka-topic_events-retry-1.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: events-retry-1
  namespace: default
  labels:
    strimzi.io/cluster: release-name-kafka
  annotations:
    nebuly.com/release-name: release-name
spec:
  topicName: events-retry-1
  partitions: 2
  replicas: 1
---
# Source: nebuly-platform/templates/kafka-topic_events-retry-2.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: events-retry-2
  namespace: default
  labels:
    strimzi.io/cluster: release-name-kafka
  annotations:
    nebuly.com/release-name: release-name
spec:
  topicName: events-retry-2
  partitions: 1
  replicas: 1
---
# Source: nebuly-platform/templates/kafka-topic_events-retry-3.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: events-retry-3
  namespace: default
  labels:
    strimzi.io/cluster: release-name-kafka
  annotations:
    nebuly.com/release-name: release-name
spec:
  topicName: events-retry-3
  partitions: 1
  replicas: 1
---
# Source: nebuly-platform/templates/kafka-topic_events-retry-dlq.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: events-dlq
  namespace: default
  labels:
    strimzi.io/cluster: release-name-kafka
  annotations:
    nebuly.com/release-name: release-name
spec:
  topicName: events-dlq
  partitions: 1
  replicas: 1
---
# Source: nebuly-platform/templates/kafka-user_main.yaml
apiVersion: kafka.strimzi.io/v1beta1
kind: KafkaUser
metadata:
  name: nebuly-platform
  namespace: default
  labels:
    strimzi.io/cluster: release-name-kafka
  annotations:
    nebuly.com/release-name: release-name
spec:
  authentication:
    type: scram-sha-512
  authorization:
    type: simple
    acls:
      - resource:
          type: topic
          name: "*"
          patternType: literal
        operations: ["All"]
        host: "*"
      - resource:
          type: group
          name: "*"
          patternType: literal
        operations: ["All"]
        host: "*"
---
# Source: nebuly-platform/templates/models-sync_job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-models-sync
  namespace: default
  labels:
    app.kubernetes.io/name: nebuly-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: nebuly-platform
    app.kubernetes.io/component: models-sync
  annotations:
    nebuly.com/release-name: release-name
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    "helm.sh/hook": post-upgrade, post-install
    "helm.sh/hook-weight": "-3"
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  backoffLimit: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: nebuly-platform
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/part-of: nebuly-platform
        app.kubernetes.io/component: models-sync
    spec:
      serviceAccountName: default
      restartPolicy: Never
      imagePullSecrets:
        - name: nebuly-docker-pull
      securityContext:
        runAsNonRoot: true
      containers:
        - name: nebuly-platform
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
          image: "ghcr.io/nebuly-ai/nebuly-models-sync:v0.4.1"
          command:
            - models-sync
            - interaction-analyzer-7b-v2:20
            - topic-classifier:6
            - action-classifier:6
            - --target-registry
            - azure_storage
          imagePullPolicy: IfNotPresent
          env:
            # -- Source registry
            - name: "SOURCE_AZURE_TENANT_ID"
              value: "4e16af92-dd38-417c-bf9d-a24f891a7987"
            - name: "SOURCE_AZURE_SUBSCRIPTION_ID"
              value: "3b2549d7-9a32-4c32-87f0-fdad07768838"
            - name: "SOURCE_AZUREML_RESOURCE_GROUP"
              value: "rg-ml-eastus-prod"
            - name: "SOURCE_AZUREML_WORKSPACE_NAME"
              value: "nblprd-aml-eastus"
            - name: "SOURCE_AZURE_CLIENT_ID"
              valueFrom:
                secretKeyRef:
                  name: nebuly-platform-credentials
                  key: nebuly-azure-client-id
            - name: "SOURCE_AZURE_CLIENT_SECRET"
              valueFrom:
                secretKeyRef:
                  name: nebuly-platform-credentials
                  key: nebuly-azure-client-secret
            # -- Target registry
            - name: "AZURE_TENANT_ID"
              value: "4e16af92-dd38-417c-bf9d-a24f891a7987"
            - name: "AZURE_CLIENT_ID"
              value: "091ca61a-1218-4ac5-94f4-02c7400d08d0"
            - name: "AZURE_STORAGE_ACCOUNT_NAME"
              value: "nbltstmodels"
            - name: "AZURE_STORAGE_CONTAINER_NAME"
              value: "models"
            
          resources:
            limits:
              memory: 8Gi
            requests:
              memory: 4Gi
          volumeMounts:
              - mountPath: /mnt/secrets-store
                name: secrets-store
                readOnly: true
      volumes:
          - csi:
              driver: secrets-store.csi.k8s.io
              readOnly: true
              volumeAttributes:
                secretProviderClass: nebuly-platform
            name: secrets-store
